#!/usr/bin/env python3
"""
Lab 02: Malware Sample Clustering - Solution

Complete implementation of malware clustering using unsupervised learning.
"""

import pandas as pd
import numpy as np
from typing import List, Tuple, Optional
from pathlib import Path

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score

import matplotlib.pyplot as plt
import seaborn as sns


# =============================================================================
# Task 1: Load Sample Data - SOLUTION
# =============================================================================

def load_malware_data(filepath: str) -> pd.DataFrame:
    """Load pre-extracted malware features."""
    df = pd.read_csv(filepath)

    # Handle missing values
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())

    # Fill categorical missing values
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        if col != 'sha256':
            df[col] = df[col].fillna('unknown')

    print(f"Loaded {len(df)} malware samples")
    return df


def explore_data(df: pd.DataFrame) -> None:
    """Print exploratory statistics about the dataset."""
    print(f"\nDataset shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")

    # Numeric statistics
    print("\nNumeric column statistics:")
    numeric_cols = ['file_size', 'entropy', 'num_imports', 'num_sections']
    for col in numeric_cols:
        if col in df.columns:
            print(f"  {col}: mean={df[col].mean():.2f}, std={df[col].std():.2f}")

    # Family distribution
    if 'family' in df.columns:
        print("\nMalware family distribution:")
        for family, count in df['family'].value_counts().items():
            print(f"  {family}: {count} ({count/len(df)*100:.1f}%)")

    # Check for missing values
    missing = df.isnull().sum()
    if missing.any():
        print("\nMissing values:")
        print(missing[missing > 0])


# =============================================================================
# Task 2: Feature Engineering - SOLUTION
# =============================================================================

def engineer_features(df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
    """Prepare features for clustering."""
    features = df.copy()
    feature_names = []

    # Numeric features
    numeric_features = ['entropy', 'num_imports', 'num_sections', 'has_debug', 'has_signature']

    # Log transform file_size (highly skewed)
    if 'file_size' in features.columns:
        features['log_file_size'] = np.log1p(features['file_size'])
        numeric_features.append('log_file_size')

    # Create feature matrix from numeric columns
    X_numeric = features[numeric_features].values
    feature_names.extend(numeric_features)

    # Handle imphash (categorical) - use frequency encoding
    if 'imphash' in features.columns:
        imphash_freq = features['imphash'].value_counts(normalize=True)
        features['imphash_freq'] = features['imphash'].map(imphash_freq)
        X_imphash = features[['imphash_freq']].values
        X_numeric = np.hstack([X_numeric, X_imphash])
        feature_names.append('imphash_freq')

    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_numeric)

    print(f"Engineered {len(feature_names)} features")
    return X_scaled, feature_names


# =============================================================================
# Task 3: Dimensionality Reduction - SOLUTION
# =============================================================================

def reduce_dimensions(X: np.ndarray, method: str = 'pca') -> np.ndarray:
    """Reduce feature dimensions for visualization."""
    if method == 'pca':
        reducer = PCA(n_components=2, random_state=42)
        X_2d = reducer.fit_transform(X)
        print(f"PCA explained variance: {sum(reducer.explained_variance_ratio_)*100:.1f}%")

    elif method == 'tsne':
        # First reduce with PCA if high-dimensional
        if X.shape[1] > 10:
            pca = PCA(n_components=min(10, X.shape[1]), random_state=42)
            X_pca = pca.fit_transform(X)
        else:
            X_pca = X

        tsne = TSNE(
            n_components=2,
            perplexity=min(30, X.shape[0] // 4),
            learning_rate=200,
            n_iter=1000,
            random_state=42
        )
        X_2d = tsne.fit_transform(X_pca)
        print("t-SNE reduction complete")

    else:
        raise ValueError(f"Unknown method: {method}")

    return X_2d


# =============================================================================
# Task 4: Clustering - SOLUTION
# =============================================================================

def find_optimal_k(X: np.ndarray, k_range: range = range(2, 10)) -> int:
    """Find optimal number of clusters using silhouette score."""
    scores = []

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        score = silhouette_score(X, labels)
        scores.append((k, score))
        print(f"  k={k}: silhouette={score:.3f}")

    optimal_k = max(scores, key=lambda x: x[1])[0]
    print(f"Optimal k: {optimal_k}")
    return optimal_k


def cluster_samples(X: np.ndarray, method: str = 'kmeans', n_clusters: int = None) -> np.ndarray:
    """Cluster malware samples."""
    if method == 'kmeans':
        if n_clusters is None:
            print("Finding optimal number of clusters...")
            n_clusters = find_optimal_k(X)

        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = model.fit_predict(X)

    elif method == 'dbscan':
        # Estimate eps using k-distance graph
        from sklearn.neighbors import NearestNeighbors
        k = min(5, X.shape[0] - 1)
        nn = NearestNeighbors(n_neighbors=k)
        nn.fit(X)
        distances, _ = nn.kneighbors(X)
        eps = np.percentile(distances[:, -1], 90)

        model = DBSCAN(eps=eps, min_samples=5)
        labels = model.fit_predict(X)

    elif method == 'hierarchical':
        if n_clusters is None:
            n_clusters = find_optimal_k(X)

        model = AgglomerativeClustering(n_clusters=n_clusters)
        labels = model.fit_predict(X)

    else:
        raise ValueError(f"Unknown method: {method}")

    return labels


# =============================================================================
# Task 5: Visualization - SOLUTION
# =============================================================================

def visualize_clusters(
    X_2d: np.ndarray,
    labels: np.ndarray,
    title: str = "Malware Clusters",
    save_path: str = None
) -> None:
    """Visualize clustering results."""
    plt.figure(figsize=(10, 8))

    # Create color palette
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    palette = sns.color_palette("husl", n_clusters)

    # Handle noise points (label -1) for DBSCAN
    unique_labels = sorted(set(labels))

    for label in unique_labels:
        mask = labels == label
        if label == -1:
            color = 'gray'
            label_name = 'Noise'
            alpha = 0.3
        else:
            color = palette[label % len(palette)]
            label_name = f'Cluster {label}'
            alpha = 0.7

        plt.scatter(
            X_2d[mask, 0],
            X_2d[mask, 1],
            c=[color],
            label=label_name,
            alpha=alpha,
            s=50
        )

    plt.title(title, fontsize=14)
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved visualization to {save_path}")

    plt.show()


def compare_with_families(
    X_2d: np.ndarray,
    cluster_labels: np.ndarray,
    family_labels: np.ndarray
) -> None:
    """Compare clustering with known malware families."""
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Left: Cluster assignments
    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
    scatter1 = axes[0].scatter(
        X_2d[:, 0], X_2d[:, 1],
        c=cluster_labels,
        cmap='tab10',
        alpha=0.7,
        s=50
    )
    axes[0].set_title("Predicted Clusters")
    axes[0].set_xlabel("Component 1")
    axes[0].set_ylabel("Component 2")
    plt.colorbar(scatter1, ax=axes[0], label='Cluster')

    # Right: Actual family labels
    scatter2 = axes[1].scatter(
        X_2d[:, 0], X_2d[:, 1],
        c=family_labels,
        cmap='tab10',
        alpha=0.7,
        s=50
    )
    axes[1].set_title("Actual Malware Families")
    axes[1].set_xlabel("Component 1")
    axes[1].set_ylabel("Component 2")
    plt.colorbar(scatter2, ax=axes[1], label='Family')

    # Calculate Adjusted Rand Index
    ari = adjusted_rand_score(family_labels, cluster_labels)
    fig.suptitle(f"Clustering vs Actual Families (ARI: {ari:.3f})", fontsize=14)

    plt.tight_layout()
    plt.show()

    print(f"\nAdjusted Rand Index: {ari:.3f}")
    print("(1.0 = perfect agreement, 0.0 = random)")


# =============================================================================
# Task 6: Analysis - SOLUTION
# =============================================================================

def analyze_clusters(df: pd.DataFrame, labels: np.ndarray) -> dict:
    """Analyze characteristics of each cluster."""
    df_analysis = df.copy()
    df_analysis['cluster'] = labels

    analysis = {}

    for cluster_id in sorted(set(labels)):
        if cluster_id == -1:
            continue  # Skip noise

        cluster_data = df_analysis[df_analysis['cluster'] == cluster_id]

        cluster_analysis = {
            'size': len(cluster_data),
            'avg_entropy': cluster_data['entropy'].mean(),
            'std_entropy': cluster_data['entropy'].std(),
            'avg_file_size': cluster_data['file_size'].mean(),
            'avg_imports': cluster_data['num_imports'].mean(),
            'avg_sections': cluster_data['num_sections'].mean(),
        }

        # Most common imphash
        if 'imphash' in cluster_data.columns:
            top_imphash = cluster_data['imphash'].value_counts().head(3)
            cluster_analysis['top_imphashes'] = top_imphash.to_dict()

        # If family labels exist, show distribution
        if 'family' in cluster_data.columns:
            family_dist = cluster_data['family'].value_counts()
            cluster_analysis['family_distribution'] = family_dist.to_dict()
            cluster_analysis['suspected_family'] = family_dist.index[0]

        analysis[cluster_id] = cluster_analysis

    return analysis


def print_cluster_report(analysis: dict) -> None:
    """Print formatted cluster analysis report."""
    print("\n" + "=" * 60)
    print("CLUSTER ANALYSIS REPORT")
    print("=" * 60)

    for cluster_id, stats in analysis.items():
        print(f"\n--- Cluster {cluster_id} ---")
        print(f"Size: {stats['size']} samples")
        print(f"Avg Entropy: {stats['avg_entropy']:.2f} (Â±{stats['std_entropy']:.2f})")
        print(f"Avg File Size: {stats['avg_file_size']/1024:.1f} KB")
        print(f"Avg Imports: {stats['avg_imports']:.1f}")
        print(f"Avg Sections: {stats['avg_sections']:.1f}")

        if 'suspected_family' in stats:
            print(f"Suspected Family: {stats['suspected_family']}")
            print("Family Distribution:")
            for family, count in stats.get('family_distribution', {}).items():
                print(f"  - {family}: {count}")


# =============================================================================
# Main Execution
# =============================================================================

def main():
    """Main execution flow."""
    print("=" * 60)
    print("Lab 02: Malware Sample Clustering - SOLUTION")
    print("=" * 60)

    # Load data
    data_path = Path(__file__).parent.parent / "data" / "malware_features.csv"

    if not data_path.exists():
        print(f"Data file not found: {data_path}")
        print("Creating sample data...")
        create_sample_data(data_path)

    print("\n[Step 1] Loading data...")
    df = load_malware_data(str(data_path))
    explore_data(df)

    # Feature engineering
    print("\n[Step 2] Engineering features...")
    X, feature_names = engineer_features(df)
    print(f"Feature matrix shape: {X.shape}")
    print(f"Features: {feature_names}")

    # Dimensionality reduction
    print("\n[Step 3] Reducing dimensions with t-SNE...")
    X_2d = reduce_dimensions(X, method='tsne')

    # Clustering
    print("\n[Step 4] Clustering samples...")
    labels = cluster_samples(X, method='kmeans')

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    print(f"Found {n_clusters} clusters")

    # Calculate silhouette score
    score = silhouette_score(X, labels)
    print(f"Silhouette Score: {score:.3f}")

    # Visualization
    print("\n[Step 5] Creating visualizations...")
    output_path = Path(__file__).parent.parent / "output"
    output_path.mkdir(exist_ok=True)
    visualize_clusters(
        X_2d, labels,
        title=f"Malware Clusters (n={n_clusters}, silhouette={score:.2f})",
        save_path=str(output_path / "cluster_visualization.png")
    )

    # Compare with known families
    if 'family' in df.columns:
        family_encoder = LabelEncoder()
        family_labels = family_encoder.fit_transform(df['family'])
        compare_with_families(X_2d, labels, family_labels)

    # Analysis
    print("\n[Step 6] Analyzing clusters...")
    analysis = analyze_clusters(df, labels)
    print_cluster_report(analysis)

    print("\n" + "=" * 60)
    print("Clustering complete!")
    print("=" * 60)


def create_sample_data(filepath: Path):
    """Create sample malware features dataset."""
    np.random.seed(42)

    families = ['emotet', 'trickbot', 'ryuk', 'cobalt_strike', 'generic']
    n_samples = 500

    data = []
    for i in range(n_samples):
        family = np.random.choice(families, p=[0.25, 0.20, 0.15, 0.15, 0.25])

        if family == 'emotet':
            file_size = np.random.lognormal(12, 0.5)
            entropy = np.random.normal(7.2, 0.3)
            num_imports = np.random.randint(30, 60)
        elif family == 'trickbot':
            file_size = np.random.lognormal(11, 0.4)
            entropy = np.random.normal(6.8, 0.4)
            num_imports = np.random.randint(40, 80)
        elif family == 'ryuk':
            file_size = np.random.lognormal(13, 0.6)
            entropy = np.random.normal(7.5, 0.2)
            num_imports = np.random.randint(20, 40)
        elif family == 'cobalt_strike':
            file_size = np.random.lognormal(10, 0.3)
            entropy = np.random.normal(7.8, 0.2)
            num_imports = np.random.randint(10, 25)
        else:
            file_size = np.random.lognormal(11.5, 0.8)
            entropy = np.random.normal(6.5, 0.6)
            num_imports = np.random.randint(20, 100)

        data.append({
            'sha256': f'sample_{i:04d}',
            'file_size': int(file_size),
            'entropy': min(8.0, max(0.0, entropy)),
            'num_imports': num_imports,
            'num_sections': np.random.randint(3, 8),
            'has_debug': np.random.choice([0, 1], p=[0.9, 0.1]),
            'has_signature': np.random.choice([0, 1], p=[0.8, 0.2]),
            'imphash': f'hash_{np.random.randint(0, 50):03d}',
            'family': family
        })

    df = pd.DataFrame(data)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(filepath, index=False)
    print(f"Created sample data with {len(df)} samples")


if __name__ == "__main__":
    main()
