{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 02: Malware Sample Clustering\n",
    "\n",
    "Use unsupervised learning to cluster malware samples by behavior and identify families.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/depalmar/ai_for_the_win/blob/main/notebooks/lab02_malware_clustering.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "- Feature extraction from malware samples\n",
    "- K-Means, DBSCAN, and hierarchical clustering\n",
    "- Dimensionality reduction (PCA, t-SNE)\n",
    "- Cluster evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "# !pip install scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Malware Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample malware feature dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Generate synthetic malware samples with different families\n",
    "families = ['Emotet', 'TrickBot', 'Ryuk', 'Dridex', 'QBot']\n",
    "data = {\n",
    "    'sha256': [f'sample_{i:04d}' for i in range(n_samples)],\n",
    "    'family': np.random.choice(families, n_samples),\n",
    "    'file_size': np.random.lognormal(12, 1.5, n_samples).astype(int),\n",
    "    'entropy': np.random.uniform(5.0, 8.0, n_samples),\n",
    "    'num_imports': np.random.randint(10, 500, n_samples),\n",
    "    'num_sections': np.random.randint(3, 12, n_samples),\n",
    "    'has_debug': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
    "    'has_signature': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "}\n",
    "\n",
    "# Add family-specific characteristics\n",
    "df = pd.DataFrame(data)\n",
    "for i, row in df.iterrows():\n",
    "    if row['family'] == 'Emotet':\n",
    "        df.loc[i, 'entropy'] = np.random.uniform(7.0, 7.8)\n",
    "        df.loc[i, 'num_imports'] = np.random.randint(200, 400)\n",
    "    elif row['family'] == 'Ryuk':\n",
    "        df.loc[i, 'file_size'] = np.random.lognormal(14, 0.5).astype(int)\n",
    "        df.loc[i, 'entropy'] = np.random.uniform(7.5, 7.99)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFamily distribution:\")\n",
    "print(df['family'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions by family\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, feature in zip(axes.flatten(), ['entropy', 'num_imports', 'file_size', 'num_sections']):\n",
    "    for family in families:\n",
    "        subset = df[df['family'] == family][feature]\n",
    "        ax.hist(subset, alpha=0.5, label=family, bins=20)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{feature} Distribution by Family')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "feature_cols = ['entropy', 'num_imports', 'num_sections', 'has_debug', 'has_signature']\n",
    "\n",
    "# Log transform file_size (highly skewed)\n",
    "df['log_file_size'] = np.log1p(df['file_size'])\n",
    "feature_cols.append('log_file_size')\n",
    "\n",
    "# Create feature matrix\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# t-SNE for better separation\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with true labels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PCA plot\n",
    "for family in families:\n",
    "    mask = df['family'] == family\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], label=family, alpha=0.7)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('PCA Projection')\n",
    "axes[0].legend()\n",
    "\n",
    "# t-SNE plot\n",
    "for family in families:\n",
    "    mask = df['family'] == family\n",
    "    axes[1].scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=family, alpha=0.7)\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].set_title('t-SNE Projection')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Clustering with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k using elbow method and silhouette score\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(k_range, inertias, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "\n",
    "axes[1].plot(k_range, silhouettes, 'go-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouettes)]\n",
    "print(f\"Optimal k based on silhouette score: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with optimal k\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "df['kmeans_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"K-Means Cluster Distribution:\")\n",
    "print(df['kmeans_cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Clustering with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "df['dbscan_cluster'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "print(\"DBSCAN Cluster Distribution:\")\n",
    "print(df['dbscan_cluster'].value_counts().sort_index())\n",
    "print(f\"\\nNoise points (label=-1): {(df['dbscan_cluster'] == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Evaluate Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode true labels\n",
    "le = LabelEncoder()\n",
    "true_labels = le.fit_transform(df['family'])\n",
    "\n",
    "# Calculate metrics\n",
    "kmeans_silhouette = silhouette_score(X_scaled, df['kmeans_cluster'])\n",
    "kmeans_ari = adjusted_rand_score(true_labels, df['kmeans_cluster'])\n",
    "\n",
    "# DBSCAN (excluding noise)\n",
    "dbscan_mask = df['dbscan_cluster'] != -1\n",
    "if dbscan_mask.sum() > 1:\n",
    "    dbscan_silhouette = silhouette_score(X_scaled[dbscan_mask], df.loc[dbscan_mask, 'dbscan_cluster'])\n",
    "    dbscan_ari = adjusted_rand_score(true_labels[dbscan_mask], df.loc[dbscan_mask, 'dbscan_cluster'])\n",
    "else:\n",
    "    dbscan_silhouette = 0\n",
    "    dbscan_ari = 0\n",
    "\n",
    "print(\"Clustering Evaluation:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"K-Means Silhouette Score: {kmeans_silhouette:.3f}\")\n",
    "print(f\"K-Means Adjusted Rand Index: {kmeans_ari:.3f}\")\n",
    "print(f\"\\nDBSCAN Silhouette Score: {dbscan_silhouette:.3f}\")\n",
    "print(f\"DBSCAN Adjusted Rand Index: {dbscan_ari:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# True labels\n",
    "for i, family in enumerate(families):\n",
    "    mask = df['family'] == family\n",
    "    axes[0].scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=family, alpha=0.7)\n",
    "axes[0].set_title('True Malware Families')\n",
    "axes[0].legend()\n",
    "\n",
    "# K-Means clusters\n",
    "scatter = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['kmeans_cluster'], cmap='viridis', alpha=0.7)\n",
    "axes[1].set_title(f'K-Means Clusters (k=5)')\n",
    "plt.colorbar(scatter, ax=axes[1])\n",
    "\n",
    "# DBSCAN clusters\n",
    "scatter = axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['dbscan_cluster'], cmap='viridis', alpha=0.7)\n",
    "axes[2].set_title('DBSCAN Clusters')\n",
    "plt.colorbar(scatter, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster composition\n",
    "print(\"Cluster Composition (K-Means):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cluster_id in sorted(df['kmeans_cluster'].unique()):\n",
    "    cluster_data = df[df['kmeans_cluster'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_data)} samples):\")\n",
    "    print(cluster_data['family'].value_counts().to_string())\n",
    "    print(f\"  Avg Entropy: {cluster_data['entropy'].mean():.2f}\")\n",
    "    print(f\"  Avg Imports: {cluster_data['num_imports'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we:\n",
    "- Extracted features from malware samples (entropy, imports, sections)\n",
    "- Applied dimensionality reduction (PCA, t-SNE) for visualization\n",
    "- Clustered samples using K-Means and DBSCAN\n",
    "- Evaluated clustering quality with silhouette score and ARI\n",
    "\n",
    "### Key Insights:\n",
    "- **High entropy** often indicates packed/encrypted malware\n",
    "- **Import patterns** can distinguish malware families\n",
    "- **t-SNE** provides better visual separation than PCA\n",
    "- **DBSCAN** can identify outliers (noise points)\n",
    "\n",
    "### Next Steps:\n",
    "1. Add more features (strings, API calls, PE headers)\n",
    "2. Try hierarchical clustering for dendrogram visualization\n",
    "3. Build a classification model using cluster labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
