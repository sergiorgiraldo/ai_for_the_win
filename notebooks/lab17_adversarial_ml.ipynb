{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 17: Adversarial Machine Learning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/depalmar/ai_for_the_win/blob/main/notebooks/lab17_adversarial_ml.ipynb)\n",
    "\n",
    "Attack and defend machine learning security models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Evasion attacks (FGSM, PGD)\n",
    "- Data poisoning attacks\n",
    "- Adversarial training\n",
    "- Robust defense strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scikit-learn matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a Simple Malware Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic malware features\n",
    "np.random.seed(42)\n",
    "\n",
    "# Features: [file_size, entropy, imports_count, suspicious_api_calls]\n",
    "n_samples = 500\n",
    "\n",
    "# Benign files\n",
    "benign = np.random.randn(n_samples // 2, 4) * [100, 0.5, 20, 2] + [500, 5, 50, 5]\n",
    "\n",
    "# Malware\n",
    "malware = np.random.randn(n_samples // 2, 4) * [50, 0.3, 10, 5] + [200, 7, 30, 20]\n",
    "\n",
    "X = np.vstack([benign, malware])\n",
    "y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Original Accuracy: {accuracy_score(y_test, clf.predict(X_test)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evasion Attack: FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, X, y, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Fast Gradient Sign Method attack.\n",
    "    Perturb features to evade detection.\n",
    "    \"\"\"\n",
    "    # Get model coefficients (gradient direction)\n",
    "    gradient = model.coef_[0]\n",
    "    \n",
    "    # For malware (y=1), we want to move towards benign classification\n",
    "    # This means moving against the gradient\n",
    "    perturbation = -np.sign(gradient) * epsilon\n",
    "    \n",
    "    # Apply perturbation only to malware samples\n",
    "    X_adv = X.copy()\n",
    "    X_adv[y == 1] += perturbation\n",
    "    \n",
    "    return X_adv\n",
    "\n",
    "# Attack the classifier\n",
    "X_test_adv = fgsm_attack(clf, X_test, y_test, epsilon=0.5)\n",
    "\n",
    "print(f\"Accuracy on original: {accuracy_score(y_test, clf.predict(X_test)):.2%}\")\n",
    "print(f\"Accuracy on adversarial: {accuracy_score(y_test, clf.predict(X_test_adv)):.2%}\")\n",
    "\n",
    "# Check evasion rate for malware\n",
    "malware_mask = y_test == 1\n",
    "original_detection = (clf.predict(X_test[malware_mask]) == 1).mean()\n",
    "adversarial_detection = (clf.predict(X_test_adv[malware_mask]) == 1).mean()\n",
    "print(f\"\\nMalware detection rate: {original_detection:.2%} -> {adversarial_detection:.2%}\")\n",
    "print(f\"Evasion success: {original_detection - adversarial_detection:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Poisoning Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_training_data(X_train, y_train, poison_rate=0.1):\n",
    "    \"\"\"\n",
    "    Poison training data by flipping labels.\n",
    "    \"\"\"\n",
    "    X_poisoned = X_train.copy()\n",
    "    y_poisoned = y_train.copy()\n",
    "    \n",
    "    n_poison = int(len(y_train) * poison_rate)\n",
    "    poison_indices = np.random.choice(len(y_train), n_poison, replace=False)\n",
    "    \n",
    "    # Flip labels\n",
    "    y_poisoned[poison_indices] = 1 - y_poisoned[poison_indices]\n",
    "    \n",
    "    return X_poisoned, y_poisoned, poison_indices\n",
    "\n",
    "# Poison the training data\n",
    "X_poisoned, y_poisoned, _ = poison_training_data(X_train, y_train, poison_rate=0.15)\n",
    "\n",
    "# Train on poisoned data\n",
    "clf_poisoned = LogisticRegression(random_state=42)\n",
    "clf_poisoned.fit(X_poisoned, y_poisoned)\n",
    "\n",
    "print(f\"Original model accuracy: {accuracy_score(y_test, clf.predict(X_test)):.2%}\")\n",
    "print(f\"Poisoned model accuracy: {accuracy_score(y_test, clf_poisoned.predict(X_test)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defense: Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(X_train, y_train, epsilon=0.3, n_iterations=3):\n",
    "    \"\"\"\n",
    "    Train model with adversarial examples included.\n",
    "    \"\"\"\n",
    "    X_aug = X_train.copy()\n",
    "    y_aug = y_train.copy()\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Train model on current data\n",
    "        model = LogisticRegression(random_state=42)\n",
    "        model.fit(X_aug, y_aug)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        X_adv = fgsm_attack(model, X_train, y_train, epsilon=epsilon)\n",
    "        \n",
    "        # Augment training data\n",
    "        X_aug = np.vstack([X_aug, X_adv])\n",
    "        y_aug = np.concatenate([y_aug, y_train])\n",
    "    \n",
    "    # Final model\n",
    "    final_model = LogisticRegression(random_state=42)\n",
    "    final_model.fit(X_aug, y_aug)\n",
    "    \n",
    "    return final_model\n",
    "\n",
    "# Train robust model\n",
    "robust_clf = adversarial_training(X_train, y_train)\n",
    "\n",
    "# Test against adversarial examples\n",
    "X_test_adv = fgsm_attack(clf, X_test, y_test, epsilon=0.5)\n",
    "\n",
    "print(\"Original Model:\")\n",
    "print(f\"  Clean accuracy: {accuracy_score(y_test, clf.predict(X_test)):.2%}\")\n",
    "print(f\"  Adversarial accuracy: {accuracy_score(y_test, clf.predict(X_test_adv)):.2%}\")\n",
    "\n",
    "print(\"\\nRobust Model (Adversarial Training):\")\n",
    "print(f\"  Clean accuracy: {accuracy_score(y_test, robust_clf.predict(X_test)):.2%}\")\n",
    "print(f\"  Adversarial accuracy: {accuracy_score(y_test, robust_clf.predict(X_test_adv)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defense: Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest is more robust to adversarial perturbations\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Defense:\")\n",
    "print(f\"  Clean accuracy: {accuracy_score(y_test, rf_clf.predict(X_test)):.2%}\")\n",
    "print(f\"  Adversarial accuracy: {accuracy_score(y_test, rf_clf.predict(X_test_adv)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model robustness at different epsilon values\n",
    "epsilons = np.linspace(0, 1, 11)\n",
    "original_acc = []\n",
    "robust_acc = []\n",
    "rf_acc = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    X_adv = fgsm_attack(clf, X_test, y_test, epsilon=eps)\n",
    "    original_acc.append(accuracy_score(y_test, clf.predict(X_adv)))\n",
    "    robust_acc.append(accuracy_score(y_test, robust_clf.predict(X_adv)))\n",
    "    rf_acc.append(accuracy_score(y_test, rf_clf.predict(X_adv)))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, original_acc, 'r-o', label='Original Model')\n",
    "plt.plot(epsilons, robust_acc, 'g-o', label='Adversarial Training')\n",
    "plt.plot(epsilons, rf_acc, 'b-o', label='Random Forest')\n",
    "plt.xlabel('Perturbation (epsilon)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Robustness to Adversarial Attacks')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Evasion Attacks**: Small perturbations can fool classifiers\n",
    "2. **Poisoning Attacks**: Corrupted training data degrades models\n",
    "3. **Adversarial Training**: Include adversarial examples in training\n",
    "4. **Ensemble Methods**: More robust than single models\n",
    "\n",
    "## Next Steps\n",
    "- **Lab 18**: Fine-Tuning for Security\n",
    "- **Lab 19**: Cloud Security AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
