{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 16: Advanced Threat Actor Profiling & Attribution\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/depalmar/ai_for_the_win/blob/main/notebooks/lab16_threat_actor_profiling.ipynb)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. **Extract and encode TTPs** using MITRE ATT&CK with weighted similarity\n",
        "2. **Build graph-based infrastructure analysis** for pivot detection\n",
        "3. **Analyze temporal patterns** to identify operational timing signatures\n",
        "4. **Implement code similarity** using fuzzy hashing (ssdeep) and YARA\n",
        "5. **Detect false flag operations** and attribution deception\n",
        "6. **Generate confidence-calibrated** attribution assessments\n",
        "7. **Apply the Diamond Model** for structured intrusion analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup\n",
        "!pip install pandas numpy scikit-learn networkx matplotlib anthropic ssdeep-py python-Levenshtein --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Set, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "import networkx as nx\n",
        "import json\n",
        "import hashlib\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Comprehensive Campaign Data\n",
        "\n",
        "Real-world attribution requires multi-dimensional data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Comprehensive campaign dataset with realistic attribution indicators\n",
        "@dataclass\n",
        "class Campaign:\n",
        "    \"\"\"Represents a threat campaign with attribution indicators.\"\"\"\n",
        "    id: str\n",
        "    name: str\n",
        "    ttps: List[str]  # MITRE ATT&CK technique IDs\n",
        "    malware_families: List[str]\n",
        "    infrastructure: Dict  # domains, IPs, certificates\n",
        "    targets: Dict  # sectors, regions, victim types\n",
        "    temporal: Dict  # timestamps, operational hours\n",
        "    code_artifacts: Dict  # imphash, ssdeep, strings, pdb paths\n",
        "    language_indicators: List[str]  # language artifacts found\n",
        "    \n",
        "CAMPAIGNS = [\n",
        "    Campaign(\n",
        "        id=\"CAMP-001\",\n",
        "        name=\"Operation ShadowStrike\",\n",
        "        ttps=[\"T1566.001\", \"T1059.001\", \"T1003.001\", \"T1071.001\", \"T1041\", \"T1567.002\"],\n",
        "        malware_families=[\"ShadowLoader\", \"CobaltStrike\"],\n",
        "        infrastructure={\n",
        "            \"domains\": [\"update-service.net\", \"cdn-assets.org\", \"secure-login.info\"],\n",
        "            \"ips\": [\"185.141.62.x\", \"91.219.236.x\"],\n",
        "            \"registrar\": \"Namecheap\",\n",
        "            \"hosting\": \"BuyVM\",\n",
        "            \"cert_issuer\": \"Let's Encrypt\",\n",
        "            \"ssl_subject\": \"*.update-service.net\"\n",
        "        },\n",
        "        targets={\n",
        "            \"sectors\": [\"Defense\", \"Aerospace\"],\n",
        "            \"regions\": [\"North America\", \"Western Europe\"],\n",
        "            \"victim_size\": \"Enterprise\"\n",
        "        },\n",
        "        temporal={\n",
        "            \"first_seen\": \"2024-01-15\",\n",
        "            \"last_seen\": \"2024-03-20\",\n",
        "            \"operational_hours\": [8, 9, 10, 11, 12, 13, 14, 15, 16],  # UTC+3\n",
        "            \"operational_days\": [0, 1, 2, 3, 4],  # Mon-Fri\n",
        "            \"activity_timestamps\": [\n",
        "                \"2024-01-15T11:23:00Z\", \"2024-01-16T09:45:00Z\",\n",
        "                \"2024-02-12T14:30:00Z\", \"2024-03-01T10:15:00Z\"\n",
        "            ]\n",
        "        },\n",
        "        code_artifacts={\n",
        "            \"imphash\": \"a3b4c5d6e7f8a1b2c3d4e5f6a7b8c9d0\",\n",
        "            \"ssdeep\": \"3072:xNQKBsL9Ck8TJHc0xNQKBsL9Ck8TJHc:xNQKBsL\",\n",
        "            \"pdb_paths\": [\"C:\\\\Users\\\\developer\\\\Desktop\\\\project\\\\Release\\\\loader.pdb\"],\n",
        "            \"unique_strings\": [\"shadowstrike_v2\", \"exfil_module_3\"],\n",
        "            \"compiler\": \"MSVC 14.0\",\n",
        "            \"linker_version\": \"14.0\"\n",
        "        },\n",
        "        language_indicators=[\"Russian keyboard layout\", \"Cyrillic error messages\"]\n",
        "    ),\n",
        "    Campaign(\n",
        "        id=\"CAMP-002\",\n",
        "        name=\"Operation NightOwl\",\n",
        "        ttps=[\"T1566.001\", \"T1059.001\", \"T1003.001\", \"T1071.001\", \"T1041\"],  # Similar TTPs\n",
        "        malware_families=[\"NightLoader\", \"CobaltStrike\"],  # Same C2 framework\n",
        "        infrastructure={\n",
        "            \"domains\": [\"software-update.net\", \"content-cdn.org\"],  # Similar naming\n",
        "            \"ips\": [\"185.141.62.x\", \"45.33.32.x\"],  # Overlapping IP range!\n",
        "            \"registrar\": \"Namecheap\",  # Same registrar\n",
        "            \"hosting\": \"BuyVM\",  # Same hosting\n",
        "            \"cert_issuer\": \"Let's Encrypt\",\n",
        "            \"ssl_subject\": \"*.software-update.net\"\n",
        "        },\n",
        "        targets={\n",
        "            \"sectors\": [\"Defense\", \"Government\"],\n",
        "            \"regions\": [\"Western Europe\", \"Eastern Europe\"],\n",
        "            \"victim_size\": \"Enterprise\"\n",
        "        },\n",
        "        temporal={\n",
        "            \"first_seen\": \"2024-02-01\",\n",
        "            \"last_seen\": \"2024-04-15\",\n",
        "            \"operational_hours\": [9, 10, 11, 12, 13, 14, 15, 16, 17],  # Similar hours\n",
        "            \"operational_days\": [0, 1, 2, 3, 4],\n",
        "            \"activity_timestamps\": [\n",
        "                \"2024-02-01T12:00:00Z\", \"2024-02-15T10:30:00Z\",\n",
        "                \"2024-03-20T15:45:00Z\", \"2024-04-01T11:20:00Z\"\n",
        "            ]\n",
        "        },\n",
        "        code_artifacts={\n",
        "            \"imphash\": \"a3b4c5d6e7f8a1b2c3d4e5f6a7b8c9d0\",  # Same imphash!\n",
        "            \"ssdeep\": \"3072:xNQKBsL9Ck8TJHc0xNQKBsL9Ck:xNQKBsL\",  # Similar ssdeep\n",
        "            \"pdb_paths\": [\"C:\\\\Users\\\\developer\\\\projects\\\\nightowl\\\\Release\\\\loader.pdb\"],\n",
        "            \"unique_strings\": [\"nightowl_module\", \"exfil_handler\"],\n",
        "            \"compiler\": \"MSVC 14.0\",\n",
        "            \"linker_version\": \"14.0\"\n",
        "        },\n",
        "        language_indicators=[\"Russian comments in code\"]\n",
        "    ),\n",
        "    Campaign(\n",
        "        id=\"CAMP-003\",\n",
        "        name=\"Operation DragonFire\",\n",
        "        ttps=[\"T1190\", \"T1505.003\", \"T1059.003\", \"T1021.002\", \"T1486\"],  # Different TTPs\n",
        "        malware_families=[\"WebShell-X\", \"RansomDragon\"],\n",
        "        infrastructure={\n",
        "            \"domains\": [\"api-gateway.cloud\", \"secure-storage.tech\"],\n",
        "            \"ips\": [\"103.224.182.x\", \"43.255.154.x\"],  # Different IP ranges\n",
        "            \"registrar\": \"GoDaddy\",\n",
        "            \"hosting\": \"Alibaba Cloud\",\n",
        "            \"cert_issuer\": \"DigiCert\",\n",
        "            \"ssl_subject\": \"api-gateway.cloud\"\n",
        "        },\n",
        "        targets={\n",
        "            \"sectors\": [\"Manufacturing\", \"Technology\"],\n",
        "            \"regions\": [\"East Asia\", \"Southeast Asia\"],\n",
        "            \"victim_size\": \"SMB\"\n",
        "        },\n",
        "        temporal={\n",
        "            \"first_seen\": \"2024-01-20\",\n",
        "            \"last_seen\": \"2024-03-30\",\n",
        "            \"operational_hours\": [1, 2, 3, 4, 5, 6, 7, 8, 9],  # Different timezone (UTC+8)\n",
        "            \"operational_days\": [0, 1, 2, 3, 4, 5],  # Includes Saturday\n",
        "            \"activity_timestamps\": [\n",
        "                \"2024-01-20T03:15:00Z\", \"2024-02-05T05:30:00Z\",\n",
        "                \"2024-03-10T02:45:00Z\", \"2024-03-30T06:00:00Z\"\n",
        "            ]\n",
        "        },\n",
        "        code_artifacts={\n",
        "            \"imphash\": \"f1e2d3c4b5a6f7e8d9c0b1a2f3e4d5c6\",\n",
        "            \"ssdeep\": \"1536:aB7cDe9FgHiJkLmN:aB7cDe9F\",\n",
        "            \"pdb_paths\": [],\n",
        "            \"unique_strings\": [\"dragon_v3\", \"ransom_encrypt\"],\n",
        "            \"compiler\": \"MinGW\",\n",
        "            \"linker_version\": \"6.3\"\n",
        "        },\n",
        "        language_indicators=[\"Simplified Chinese strings\", \"Chinese keyboard layout\"]\n",
        "    ),\n",
        "    Campaign(\n",
        "        id=\"CAMP-004\",\n",
        "        name=\"Operation FalseFlag\",  # Potential false flag operation\n",
        "        ttps=[\"T1566.001\", \"T1059.001\", \"T1003.001\", \"T1071.001\"],  # Mimics CAMP-001\n",
        "        malware_families=[\"FakeShadow\"],  # Mimics naming\n",
        "        infrastructure={\n",
        "            \"domains\": [\"update-services.net\"],  # Very similar to CAMP-001\n",
        "            \"ips\": [\"198.51.100.x\"],  # Different infrastructure\n",
        "            \"registrar\": \"NameSilo\",\n",
        "            \"hosting\": \"DigitalOcean\",\n",
        "            \"cert_issuer\": \"Let's Encrypt\",\n",
        "            \"ssl_subject\": \"update-services.net\"\n",
        "        },\n",
        "        targets={\n",
        "            \"sectors\": [\"Defense\"],\n",
        "            \"regions\": [\"Eastern Europe\"],  # Different target region\n",
        "            \"victim_size\": \"Enterprise\"\n",
        "        },\n",
        "        temporal={\n",
        "            \"first_seen\": \"2024-03-01\",\n",
        "            \"last_seen\": \"2024-03-15\",\n",
        "            \"operational_hours\": [14, 15, 16, 17, 18, 19, 20],  # Different hours (appears UTC+8)\n",
        "            \"operational_days\": [0, 1, 2, 3, 4],\n",
        "            \"activity_timestamps\": [\n",
        "                \"2024-03-01T16:00:00Z\", \"2024-03-05T18:30:00Z\",\n",
        "                \"2024-03-10T15:15:00Z\"\n",
        "            ]\n",
        "        },\n",
        "        code_artifacts={\n",
        "            \"imphash\": \"d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9\",  # Different imphash\n",
        "            \"ssdeep\": \"768:qR5tU7vW9xY1zA3bC5dE:qR5tU7v\",\n",
        "            \"pdb_paths\": [\"D:\\\\projects\\\\fake_shadow\\\\Release\\\\payload.pdb\"],\n",
        "            \"unique_strings\": [\"shadowstrike_v2\"],  # Copied string from CAMP-001!\n",
        "            \"compiler\": \"Clang\",  # Different compiler\n",
        "            \"linker_version\": \"12.0\"\n",
        "        },\n",
        "        language_indicators=[\"Intentional Cyrillic strings\", \"But UTF-8 BOM typical of Western tools\"]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Loaded campaigns:\")\n",
        "for c in CAMPAIGNS:\n",
        "    print(f\"  {c.id}: {c.name} - {len(c.ttps)} TTPs, {len(c.malware_families)} malware families\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Known Threat Actor Database\n",
        "\n",
        "Reference profiles for attribution matching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@dataclass\n",
        "class ThreatActor:\n",
        "    \"\"\"Known threat actor profile.\"\"\"\n",
        "    name: str\n",
        "    aliases: List[str]\n",
        "    origin: str\n",
        "    motivation: str\n",
        "    active_since: str\n",
        "    signature_ttps: List[str]\n",
        "    preferred_malware: List[str]\n",
        "    target_sectors: List[str]\n",
        "    target_regions: List[str]\n",
        "    infrastructure_patterns: Dict\n",
        "    operational_tempo: Dict\n",
        "    code_signatures: Dict\n",
        "    confidence_indicators: List[str]\n",
        "\n",
        "KNOWN_ACTORS = {\n",
        "    \"APT28\": ThreatActor(\n",
        "        name=\"APT28\",\n",
        "        aliases=[\"Fancy Bear\", \"Sofacy\", \"Sednit\", \"STRONTIUM\"],\n",
        "        origin=\"Russia\",\n",
        "        motivation=\"Espionage\",\n",
        "        active_since=\"2007\",\n",
        "        signature_ttps=[\"T1566.001\", \"T1566.002\", \"T1059.001\", \"T1003.001\", \"T1071.001\", \"T1041\"],\n",
        "        preferred_malware=[\"X-Agent\", \"Zebrocy\", \"CobaltStrike\", \"Komplex\"],\n",
        "        target_sectors=[\"Government\", \"Defense\", \"Aerospace\", \"Media\"],\n",
        "        target_regions=[\"North America\", \"Western Europe\", \"Eastern Europe\"],\n",
        "        infrastructure_patterns={\n",
        "            \"registrars\": [\"Namecheap\", \"PDR Ltd\"],\n",
        "            \"hosting\": [\"BuyVM\", \"Linode\"],\n",
        "            \"domain_themes\": [\"update\", \"service\", \"cdn\", \"software\"],\n",
        "            \"ip_ranges\": [\"185.141.x.x\", \"91.219.x.x\"]\n",
        "        },\n",
        "        operational_tempo={\n",
        "            \"timezone_offset\": 3,  # UTC+3 (Moscow)\n",
        "            \"working_hours\": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
        "            \"working_days\": [0, 1, 2, 3, 4]  # Mon-Fri\n",
        "        },\n",
        "        code_signatures={\n",
        "            \"compilers\": [\"MSVC 14.0\", \"MSVC 19.0\"],\n",
        "            \"language_artifacts\": [\"Russian\", \"Cyrillic\"]\n",
        "        },\n",
        "        confidence_indicators=[\n",
        "            \"Use of X-Agent malware\",\n",
        "            \"OAuth phishing campaigns\",\n",
        "            \"Targeting NATO members\",\n",
        "            \"Moscow working hours\"\n",
        "        ]\n",
        "    ),\n",
        "    \"APT41\": ThreatActor(\n",
        "        name=\"APT41\",\n",
        "        aliases=[\"Winnti\", \"Barium\", \"Wicked Panda\"],\n",
        "        origin=\"China\",\n",
        "        motivation=\"Espionage + Financial\",\n",
        "        active_since=\"2012\",\n",
        "        signature_ttps=[\"T1190\", \"T1505.003\", \"T1059.003\", \"T1021.002\", \"T1486\"],\n",
        "        preferred_malware=[\"ShadowPad\", \"Winnti\", \"PlugX\", \"CobaltStrike\"],\n",
        "        target_sectors=[\"Technology\", \"Gaming\", \"Healthcare\", \"Manufacturing\"],\n",
        "        target_regions=[\"East Asia\", \"Southeast Asia\", \"North America\"],\n",
        "        infrastructure_patterns={\n",
        "            \"registrars\": [\"GoDaddy\", \"Alibaba\"],\n",
        "            \"hosting\": [\"Alibaba Cloud\", \"Tencent Cloud\"],\n",
        "            \"domain_themes\": [\"api\", \"gateway\", \"cloud\", \"storage\"],\n",
        "            \"ip_ranges\": [\"103.x.x.x\", \"43.x.x.x\"]\n",
        "        },\n",
        "        operational_tempo={\n",
        "            \"timezone_offset\": 8,  # UTC+8 (Beijing)\n",
        "            \"working_hours\": [1, 2, 3, 4, 5, 6, 7, 8, 9],  # When it's 9-17 in Beijing\n",
        "            \"working_days\": [0, 1, 2, 3, 4, 5]  # Mon-Sat\n",
        "        },\n",
        "        code_signatures={\n",
        "            \"compilers\": [\"MinGW\", \"Clang\"],\n",
        "            \"language_artifacts\": [\"Chinese\", \"Simplified Chinese\"]\n",
        "        },\n",
        "        confidence_indicators=[\n",
        "            \"Use of ShadowPad malware\",\n",
        "            \"Supply chain compromises\",\n",
        "            \"Gaming industry targeting\",\n",
        "            \"Beijing working hours\"\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"Known Threat Actors:\")\n",
        "for name, actor in KNOWN_ACTORS.items():\n",
        "    print(f\"  {name} ({actor.origin}): {actor.motivation}\")\n",
        "    print(f\"    Signature TTPs: {len(actor.signature_ttps)}, Target Sectors: {actor.target_sectors}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Advanced TTP Analysis with Weighted Similarity\n",
        "\n",
        "Not all TTPs are equal for attribution - rare techniques are more distinctive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class AdvancedTTPAnalyzer:\n",
        "    \"\"\"TTP analysis with weighted similarity based on technique rarity.\"\"\"\n",
        "    \n",
        "    # TTP weights based on attribution value (higher = more distinctive)\n",
        "    TTP_WEIGHTS = {\n",
        "        # Very High - Custom/rare techniques\n",
        "        \"T1055.012\": 3.0,  # Process Hollowing\n",
        "        \"T1127.001\": 3.0,  # MSBuild\n",
        "        \"T1218.011\": 3.0,  # Rundll32\n",
        "        \n",
        "        # High - Distinctive techniques\n",
        "        \"T1003.001\": 2.0,  # LSASS Memory\n",
        "        \"T1041\": 2.0,      # Exfiltration Over C2\n",
        "        \"T1567.002\": 2.0,  # Exfil to Cloud Storage\n",
        "        \"T1486\": 2.0,      # Data Encrypted for Impact\n",
        "        \"T1505.003\": 2.0,  # Web Shell\n",
        "        \n",
        "        # Medium - Common but still useful\n",
        "        \"T1071.001\": 1.5,  # Web Protocols\n",
        "        \"T1021.002\": 1.5,  # SMB/Windows Admin Shares\n",
        "        \"T1190\": 1.5,      # Exploit Public-Facing App\n",
        "        \n",
        "        # Low - Very common techniques\n",
        "        \"T1566.001\": 1.0,  # Spearphishing Attachment\n",
        "        \"T1566.002\": 1.0,  # Spearphishing Link\n",
        "        \"T1059.001\": 1.0,  # PowerShell\n",
        "        \"T1059.003\": 1.0,  # Windows Command Shell\n",
        "    }\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.all_ttps = list(self.TTP_WEIGHTS.keys())\n",
        "    \n",
        "    def get_weight(self, ttp: str) -> float:\n",
        "        \"\"\"Get attribution weight for a TTP.\"\"\"\n",
        "        return self.TTP_WEIGHTS.get(ttp, 1.0)\n",
        "    \n",
        "    def weighted_jaccard(self, ttps_a: List[str], ttps_b: List[str]) -> float:\n",
        "        \"\"\"Calculate weighted Jaccard similarity.\"\"\"\n",
        "        set_a, set_b = set(ttps_a), set(ttps_b)\n",
        "        \n",
        "        if not set_a or not set_b:\n",
        "            return 0.0\n",
        "        \n",
        "        intersection = set_a & set_b\n",
        "        union = set_a | set_b\n",
        "        \n",
        "        # Weighted intersection / weighted union\n",
        "        weighted_intersection = sum(self.get_weight(t) for t in intersection)\n",
        "        weighted_union = sum(self.get_weight(t) for t in union)\n",
        "        \n",
        "        return weighted_intersection / weighted_union if weighted_union > 0 else 0.0\n",
        "    \n",
        "    def calculate_ttp_fingerprint(self, ttps: List[str]) -> Dict:\n",
        "        \"\"\"Generate a distinctive TTP fingerprint.\"\"\"\n",
        "        return {\n",
        "            \"total_ttps\": len(ttps),\n",
        "            \"unique_ttps\": len(set(ttps)),\n",
        "            \"high_value_ttps\": [t for t in ttps if self.get_weight(t) >= 2.0],\n",
        "            \"weighted_score\": sum(self.get_weight(t) for t in ttps),\n",
        "            \"tactic_coverage\": self._calculate_tactic_coverage(ttps)\n",
        "        }\n",
        "    \n",
        "    def _calculate_tactic_coverage(self, ttps: List[str]) -> Dict:\n",
        "        \"\"\"Map TTPs to tactics for kill chain analysis.\"\"\"\n",
        "        tactic_map = {\n",
        "            \"initial_access\": [\"T1566.001\", \"T1566.002\", \"T1190\"],\n",
        "            \"execution\": [\"T1059.001\", \"T1059.003\"],\n",
        "            \"persistence\": [\"T1505.003\"],\n",
        "            \"credential_access\": [\"T1003.001\"],\n",
        "            \"lateral_movement\": [\"T1021.002\"],\n",
        "            \"exfiltration\": [\"T1041\", \"T1567.002\"],\n",
        "            \"impact\": [\"T1486\"]\n",
        "        }\n",
        "        \n",
        "        coverage = {}\n",
        "        for tactic, tactic_ttps in tactic_map.items():\n",
        "            matches = [t for t in ttps if t in tactic_ttps]\n",
        "            coverage[tactic] = len(matches) > 0\n",
        "        \n",
        "        return coverage\n",
        "\n",
        "\n",
        "# Test TTP analysis\n",
        "ttp_analyzer = AdvancedTTPAnalyzer()\n",
        "\n",
        "print(\"TTP Fingerprints:\")\n",
        "for campaign in CAMPAIGNS[:2]:\n",
        "    fp = ttp_analyzer.calculate_ttp_fingerprint(campaign.ttps)\n",
        "    print(f\"\\n{campaign.name}:\")\n",
        "    print(f\"  High-value TTPs: {fp['high_value_ttps']}\")\n",
        "    print(f\"  Weighted Score: {fp['weighted_score']:.1f}\")\n",
        "\n",
        "# Compare campaigns\n",
        "print(\"\\nWeighted TTP Similarity Matrix:\")\n",
        "for i, c1 in enumerate(CAMPAIGNS):\n",
        "    sims = []\n",
        "    for c2 in CAMPAIGNS:\n",
        "        sim = ttp_analyzer.weighted_jaccard(c1.ttps, c2.ttps)\n",
        "        sims.append(f\"{sim:.2f}\")\n",
        "    print(f\"  {c1.id}: [{', '.join(sims)}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Graph-Based Infrastructure Analysis\n",
        "\n",
        "Identify infrastructure overlaps and pivots between campaigns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class InfrastructureGraphAnalyzer:\n",
        "    \"\"\"Graph-based infrastructure analysis for attribution.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.graph = nx.Graph()\n",
        "    \n",
        "    def build_infrastructure_graph(self, campaigns: List[Campaign]) -> nx.Graph:\n",
        "        \"\"\"Build a graph connecting campaigns through shared infrastructure.\"\"\"\n",
        "        self.graph = nx.Graph()\n",
        "        \n",
        "        for campaign in campaigns:\n",
        "            # Add campaign node\n",
        "            self.graph.add_node(campaign.id, type=\"campaign\", name=campaign.name)\n",
        "            \n",
        "            infra = campaign.infrastructure\n",
        "            \n",
        "            # Add infrastructure nodes and edges\n",
        "            for domain in infra.get(\"domains\", []):\n",
        "                self.graph.add_node(domain, type=\"domain\")\n",
        "                self.graph.add_edge(campaign.id, domain, relation=\"uses_domain\")\n",
        "            \n",
        "            for ip in infra.get(\"ips\", []):\n",
        "                self.graph.add_node(ip, type=\"ip\")\n",
        "                self.graph.add_edge(campaign.id, ip, relation=\"uses_ip\")\n",
        "            \n",
        "            # Add registrar\n",
        "            registrar = infra.get(\"registrar\")\n",
        "            if registrar:\n",
        "                self.graph.add_node(registrar, type=\"registrar\")\n",
        "                self.graph.add_edge(campaign.id, registrar, relation=\"uses_registrar\")\n",
        "            \n",
        "            # Add hosting provider\n",
        "            hosting = infra.get(\"hosting\")\n",
        "            if hosting:\n",
        "                self.graph.add_node(hosting, type=\"hosting\")\n",
        "                self.graph.add_edge(campaign.id, hosting, relation=\"uses_hosting\")\n",
        "        \n",
        "        return self.graph\n",
        "    \n",
        "    def find_infrastructure_overlaps(self) -> List[Dict]:\n",
        "        \"\"\"Find campaigns with shared infrastructure.\"\"\"\n",
        "        overlaps = []\n",
        "        campaign_nodes = [n for n, d in self.graph.nodes(data=True) if d.get('type') == 'campaign']\n",
        "        \n",
        "        for i, c1 in enumerate(campaign_nodes):\n",
        "            for c2 in campaign_nodes[i+1:]:\n",
        "                # Find common neighbors (shared infrastructure)\n",
        "                c1_neighbors = set(self.graph.neighbors(c1))\n",
        "                c2_neighbors = set(self.graph.neighbors(c2))\n",
        "                shared = c1_neighbors & c2_neighbors\n",
        "                \n",
        "                if shared:\n",
        "                    shared_details = []\n",
        "                    for node in shared:\n",
        "                        node_type = self.graph.nodes[node].get('type', 'unknown')\n",
        "                        shared_details.append({\"node\": node, \"type\": node_type})\n",
        "                    \n",
        "                    overlaps.append({\n",
        "                        \"campaign_1\": c1,\n",
        "                        \"campaign_2\": c2,\n",
        "                        \"shared_infrastructure\": shared_details,\n",
        "                        \"overlap_count\": len(shared),\n",
        "                        \"overlap_score\": len(shared) / min(len(c1_neighbors), len(c2_neighbors))\n",
        "                    })\n",
        "        \n",
        "        return sorted(overlaps, key=lambda x: x['overlap_score'], reverse=True)\n",
        "    \n",
        "    def identify_infrastructure_clusters(self) -> List[Set]:\n",
        "        \"\"\"Find clusters of related campaigns based on infrastructure.\"\"\"\n",
        "        # Project bipartite graph to campaign-only graph\n",
        "        campaign_nodes = [n for n, d in self.graph.nodes(data=True) if d.get('type') == 'campaign']\n",
        "        \n",
        "        # Create campaign similarity graph\n",
        "        campaign_graph = nx.Graph()\n",
        "        campaign_graph.add_nodes_from(campaign_nodes)\n",
        "        \n",
        "        overlaps = self.find_infrastructure_overlaps()\n",
        "        for overlap in overlaps:\n",
        "            if overlap['overlap_score'] > 0.2:  # Threshold\n",
        "                campaign_graph.add_edge(\n",
        "                    overlap['campaign_1'],\n",
        "                    overlap['campaign_2'],\n",
        "                    weight=overlap['overlap_score']\n",
        "                )\n",
        "        \n",
        "        # Find connected components (clusters)\n",
        "        clusters = list(nx.connected_components(campaign_graph))\n",
        "        return clusters\n",
        "\n",
        "\n",
        "# Analyze infrastructure\n",
        "infra_analyzer = InfrastructureGraphAnalyzer()\n",
        "infra_analyzer.build_infrastructure_graph(CAMPAIGNS)\n",
        "\n",
        "print(\"Infrastructure Overlaps:\")\n",
        "overlaps = infra_analyzer.find_infrastructure_overlaps()\n",
        "for overlap in overlaps:\n",
        "    print(f\"\\n{overlap['campaign_1']} <-> {overlap['campaign_2']}:\")\n",
        "    print(f\"  Overlap Score: {overlap['overlap_score']:.2%}\")\n",
        "    for shared in overlap['shared_infrastructure']:\n",
        "        print(f\"    - {shared['type']}: {shared['node']}\")\n",
        "\n",
        "print(\"\\nInfrastructure-Based Clusters:\")\n",
        "clusters = infra_analyzer.identify_infrastructure_clusters()\n",
        "for i, cluster in enumerate(clusters):\n",
        "    print(f\"  Cluster {i+1}: {cluster}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Temporal Pattern Analysis\n",
        "\n",
        "Operational timing reveals actor working hours and timezone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TemporalAnalyzer:\n",
        "    \"\"\"Analyze temporal patterns for attribution.\"\"\"\n",
        "    \n",
        "    def extract_temporal_fingerprint(self, campaign: Campaign) -> Dict:\n",
        "        \"\"\"Extract temporal fingerprint from campaign activity.\"\"\"\n",
        "        timestamps = campaign.temporal.get(\"activity_timestamps\", [])\n",
        "        \n",
        "        if not timestamps:\n",
        "            return {}\n",
        "        \n",
        "        # Parse timestamps\n",
        "        datetimes = [datetime.fromisoformat(ts.replace('Z', '+00:00')) for ts in timestamps]\n",
        "        \n",
        "        # Extract hours (UTC)\n",
        "        hours = [dt.hour for dt in datetimes]\n",
        "        days = [dt.weekday() for dt in datetimes]\n",
        "        \n",
        "        # Estimate timezone based on working hours assumption (9-17 local)\n",
        "        avg_hour = np.mean(hours)\n",
        "        estimated_tz = self._estimate_timezone(avg_hour)\n",
        "        \n",
        "        return {\n",
        "            \"utc_hours\": hours,\n",
        "            \"days_of_week\": days,\n",
        "            \"average_hour_utc\": avg_hour,\n",
        "            \"estimated_timezone\": estimated_tz,\n",
        "            \"working_pattern\": self._classify_working_pattern(hours, days),\n",
        "            \"activity_span_days\": (max(datetimes) - min(datetimes)).days\n",
        "        }\n",
        "    \n",
        "    def _estimate_timezone(self, avg_utc_hour: float) -> str:\n",
        "        \"\"\"Estimate timezone assuming 9-17 working hours.\"\"\"\n",
        "        # If avg activity is at 12 UTC, and we assume noon local, offset is 0\n",
        "        # If avg activity is at 6 UTC, could be noon at UTC+6\n",
        "        assumed_local_noon = 12\n",
        "        offset = assumed_local_noon - avg_utc_hour\n",
        "        \n",
        "        # Round to common timezone\n",
        "        if 2 <= offset <= 4:\n",
        "            return \"UTC+3 (Moscow/Eastern Europe)\"\n",
        "        elif 7 <= offset <= 9:\n",
        "            return \"UTC+8 (Beijing/Singapore)\"\n",
        "        elif -6 <= offset <= -4:\n",
        "            return \"UTC-5 (US Eastern)\"\n",
        "        elif -9 <= offset <= -7:\n",
        "            return \"UTC-8 (US Pacific)\"\n",
        "        else:\n",
        "            return f\"UTC{int(offset):+d}\"\n",
        "    \n",
        "    def _classify_working_pattern(self, hours: List[int], days: List[int]) -> str:\n",
        "        \"\"\"Classify working pattern.\"\"\"\n",
        "        weekend_activity = sum(1 for d in days if d >= 5)\n",
        "        total = len(days)\n",
        "        \n",
        "        if weekend_activity / total > 0.3:\n",
        "            return \"6-day work week (common in Asia)\"\n",
        "        elif weekend_activity == 0:\n",
        "            return \"5-day work week (Western pattern)\"\n",
        "        else:\n",
        "            return \"Mixed pattern\"\n",
        "    \n",
        "    def calculate_temporal_similarity(self, fp1: Dict, fp2: Dict) -> float:\n",
        "        \"\"\"Calculate similarity between temporal fingerprints.\"\"\"\n",
        "        if not fp1 or not fp2:\n",
        "            return 0.0\n",
        "        \n",
        "        score = 0.0\n",
        "        \n",
        "        # Hour similarity (closer hours = higher score)\n",
        "        hour_diff = abs(fp1['average_hour_utc'] - fp2['average_hour_utc'])\n",
        "        hour_sim = 1.0 - min(hour_diff / 12, 1.0)  # Max 12 hour difference\n",
        "        score += hour_sim * 0.5\n",
        "        \n",
        "        # Working pattern match\n",
        "        if fp1['working_pattern'] == fp2['working_pattern']:\n",
        "            score += 0.3\n",
        "        \n",
        "        # Timezone match\n",
        "        if fp1['estimated_timezone'] == fp2['estimated_timezone']:\n",
        "            score += 0.2\n",
        "        \n",
        "        return score\n",
        "\n",
        "\n",
        "# Analyze temporal patterns\n",
        "temporal_analyzer = TemporalAnalyzer()\n",
        "\n",
        "print(\"Temporal Fingerprints:\")\n",
        "temporal_fps = {}\n",
        "for campaign in CAMPAIGNS:\n",
        "    fp = temporal_analyzer.extract_temporal_fingerprint(campaign)\n",
        "    temporal_fps[campaign.id] = fp\n",
        "    print(f\"\\n{campaign.name}:\")\n",
        "    print(f\"  Estimated Timezone: {fp.get('estimated_timezone', 'Unknown')}\")\n",
        "    print(f\"  Working Pattern: {fp.get('working_pattern', 'Unknown')}\")\n",
        "    print(f\"  Activity Hours (UTC): {fp.get('utc_hours', [])}\")\n",
        "\n",
        "print(\"\\nTemporal Similarity (CAMP-001 vs others):\")\n",
        "fp1 = temporal_fps['CAMP-001']\n",
        "for cid, fp2 in temporal_fps.items():\n",
        "    sim = temporal_analyzer.calculate_temporal_similarity(fp1, fp2)\n",
        "    print(f\"  CAMP-001 <-> {cid}: {sim:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Code Similarity & Malware Attribution\n",
        "\n",
        "Deep code analysis for attribution indicators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CodeSimilarityAnalyzer:\n",
        "    \"\"\"Analyze code similarities for attribution.\"\"\"\n",
        "    \n",
        "    def calculate_code_similarity(self, c1: Campaign, c2: Campaign) -> Dict:\n",
        "        \"\"\"Calculate multi-dimensional code similarity.\"\"\"\n",
        "        art1 = c1.code_artifacts\n",
        "        art2 = c2.code_artifacts\n",
        "        \n",
        "        scores = {}\n",
        "        \n",
        "        # Imphash exact match (very strong indicator)\n",
        "        if art1.get('imphash') and art2.get('imphash'):\n",
        "            scores['imphash_match'] = 1.0 if art1['imphash'] == art2['imphash'] else 0.0\n",
        "        \n",
        "        # SSDeep fuzzy hash similarity\n",
        "        if art1.get('ssdeep') and art2.get('ssdeep'):\n",
        "            scores['ssdeep_similarity'] = self._ssdeep_compare(art1['ssdeep'], art2['ssdeep'])\n",
        "        \n",
        "        # Compiler match\n",
        "        if art1.get('compiler') and art2.get('compiler'):\n",
        "            scores['compiler_match'] = 1.0 if art1['compiler'] == art2['compiler'] else 0.0\n",
        "        \n",
        "        # Unique string overlap\n",
        "        strings1 = set(art1.get('unique_strings', []))\n",
        "        strings2 = set(art2.get('unique_strings', []))\n",
        "        if strings1 and strings2:\n",
        "            overlap = strings1 & strings2\n",
        "            scores['string_overlap'] = len(overlap) / min(len(strings1), len(strings2))\n",
        "            scores['shared_strings'] = list(overlap)\n",
        "        \n",
        "        # PDB path analysis\n",
        "        pdb1 = art1.get('pdb_paths', [])\n",
        "        pdb2 = art2.get('pdb_paths', [])\n",
        "        if pdb1 and pdb2:\n",
        "            scores['pdb_similarity'] = self._analyze_pdb_paths(pdb1, pdb2)\n",
        "        \n",
        "        # Calculate weighted score\n",
        "        weights = {\n",
        "            'imphash_match': 0.35,\n",
        "            'ssdeep_similarity': 0.25,\n",
        "            'string_overlap': 0.20,\n",
        "            'compiler_match': 0.10,\n",
        "            'pdb_similarity': 0.10\n",
        "        }\n",
        "        \n",
        "        weighted_score = sum(\n",
        "            scores.get(k, 0) * v \n",
        "            for k, v in weights.items()\n",
        "            if k in scores\n",
        "        )\n",
        "        scores['weighted_total'] = weighted_score\n",
        "        \n",
        "        return scores\n",
        "    \n",
        "    def _ssdeep_compare(self, hash1: str, hash2: str) -> float:\n",
        "        \"\"\"Simplified ssdeep comparison (0-1 score).\"\"\"\n",
        "        # In real implementation, use ssdeep.compare()\n",
        "        # Here we do a simple prefix match\n",
        "        prefix_len = min(len(hash1), len(hash2), 20)\n",
        "        matches = sum(1 for a, b in zip(hash1[:prefix_len], hash2[:prefix_len]) if a == b)\n",
        "        return matches / prefix_len\n",
        "    \n",
        "    def _analyze_pdb_paths(self, paths1: List[str], paths2: List[str]) -> float:\n",
        "        \"\"\"Analyze PDB path similarity.\"\"\"\n",
        "        for p1 in paths1:\n",
        "            for p2 in paths2:\n",
        "                # Check for same username\n",
        "                user1 = self._extract_username(p1)\n",
        "                user2 = self._extract_username(p2)\n",
        "                if user1 and user2 and user1 == user2:\n",
        "                    return 1.0\n",
        "                \n",
        "                # Check for similar structure\n",
        "                if self._similar_path_structure(p1, p2):\n",
        "                    return 0.5\n",
        "        return 0.0\n",
        "    \n",
        "    def _extract_username(self, pdb_path: str) -> Optional[str]:\n",
        "        \"\"\"Extract username from PDB path.\"\"\"\n",
        "        match = re.search(r'Users\\\\([^\\\\]+)\\\\', pdb_path)\n",
        "        return match.group(1) if match else None\n",
        "    \n",
        "    def _similar_path_structure(self, p1: str, p2: str) -> bool:\n",
        "        \"\"\"Check if paths have similar structure.\"\"\"\n",
        "        parts1 = p1.lower().split('\\\\')\n",
        "        parts2 = p2.lower().split('\\\\')\n",
        "        \n",
        "        # Check for common project structure indicators\n",
        "        common = set(parts1) & set(parts2)\n",
        "        return len(common) >= 3\n",
        "\n",
        "\n",
        "# Analyze code similarities\n",
        "code_analyzer = CodeSimilarityAnalyzer()\n",
        "\n",
        "print(\"Code Similarity Analysis:\")\n",
        "for i, c1 in enumerate(CAMPAIGNS):\n",
        "    for c2 in CAMPAIGNS[i+1:]:\n",
        "        sim = code_analyzer.calculate_code_similarity(c1, c2)\n",
        "        if sim['weighted_total'] > 0.2:  # Only show significant similarities\n",
        "            print(f\"\\n{c1.id} <-> {c2.id}: Score={sim['weighted_total']:.2f}\")\n",
        "            if sim.get('imphash_match'):\n",
        "                print(f\"  ⚠️  IMPHASH MATCH (very strong indicator)\")\n",
        "            if sim.get('shared_strings'):\n",
        "                print(f\"  Shared strings: {sim['shared_strings']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: False Flag Detection\n",
        "\n",
        "Identify potential deception and attribution manipulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class FalseFlagDetector:\n",
        "    \"\"\"Detect potential false flag operations.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.indicators = []\n",
        "    \n",
        "    def analyze_for_false_flags(self, campaign: Campaign, \n",
        "                                 similar_campaigns: List[Campaign],\n",
        "                                 temporal_fp: Dict,\n",
        "                                 code_sim: Dict) -> Dict:\n",
        "        \"\"\"Analyze campaign for false flag indicators.\"\"\"\n",
        "        flags = []\n",
        "        confidence_reduction = 0.0\n",
        "        \n",
        "        # 1. Check for copied but not identical code\n",
        "        if code_sim.get('shared_strings') and not code_sim.get('imphash_match'):\n",
        "            flags.append({\n",
        "                \"type\": \"copied_strings\",\n",
        "                \"description\": \"Unique strings copied but different binary\",\n",
        "                \"severity\": \"high\"\n",
        "            })\n",
        "            confidence_reduction += 0.2\n",
        "        \n",
        "        # 2. Check for inconsistent temporal patterns\n",
        "        if temporal_fp:\n",
        "            for sim_camp in similar_campaigns:\n",
        "                sim_tf = TemporalAnalyzer().extract_temporal_fingerprint(sim_camp)\n",
        "                if sim_tf:\n",
        "                    hour_diff = abs(temporal_fp.get('average_hour_utc', 0) - \n",
        "                                   sim_tf.get('average_hour_utc', 0))\n",
        "                    if hour_diff > 6:  # Different timezone\n",
        "                        flags.append({\n",
        "                            \"type\": \"temporal_inconsistency\",\n",
        "                            \"description\": f\"Operating hours differ by {hour_diff:.0f}h from similar campaign\",\n",
        "                            \"severity\": \"high\"\n",
        "                        })\n",
        "                        confidence_reduction += 0.15\n",
        "        \n",
        "        # 3. Check for language indicator inconsistencies\n",
        "        if campaign.language_indicators:\n",
        "            mixed_languages = len(set(\n",
        "                ind.split()[0] for ind in campaign.language_indicators \n",
        "                if ind\n",
        "            ))\n",
        "            if mixed_languages > 1:\n",
        "                flags.append({\n",
        "                    \"type\": \"mixed_language_artifacts\",\n",
        "                    \"description\": \"Multiple language indicators (possible planting)\",\n",
        "                    \"severity\": \"medium\"\n",
        "                })\n",
        "                confidence_reduction += 0.1\n",
        "        \n",
        "        # 4. Check for infrastructure inconsistency\n",
        "        if similar_campaigns:\n",
        "            sim_infra = similar_campaigns[0].infrastructure\n",
        "            camp_infra = campaign.infrastructure\n",
        "            \n",
        "            # Similar domain naming but different hosting\n",
        "            if (self._similar_domain_naming(sim_infra, camp_infra) and \n",
        "                camp_infra.get('hosting') != sim_infra.get('hosting')):\n",
        "                flags.append({\n",
        "                    \"type\": \"infrastructure_inconsistency\",\n",
        "                    \"description\": \"Similar domain naming but different infrastructure provider\",\n",
        "                    \"severity\": \"medium\"\n",
        "                })\n",
        "                confidence_reduction += 0.1\n",
        "        \n",
        "        # 5. Check for compiler mismatch\n",
        "        if similar_campaigns:\n",
        "            sim_compiler = similar_campaigns[0].code_artifacts.get('compiler')\n",
        "            camp_compiler = campaign.code_artifacts.get('compiler')\n",
        "            if sim_compiler and camp_compiler and sim_compiler != camp_compiler:\n",
        "                flags.append({\n",
        "                    \"type\": \"toolchain_mismatch\",\n",
        "                    \"description\": f\"Different compiler ({camp_compiler} vs {sim_compiler})\",\n",
        "                    \"severity\": \"medium\"\n",
        "                })\n",
        "                confidence_reduction += 0.1\n",
        "        \n",
        "        return {\n",
        "            \"campaign\": campaign.id,\n",
        "            \"false_flag_indicators\": flags,\n",
        "            \"indicator_count\": len(flags),\n",
        "            \"confidence_reduction\": min(confidence_reduction, 0.5),\n",
        "            \"assessment\": self._assess_false_flag_likelihood(len(flags))\n",
        "        }\n",
        "    \n",
        "    def _similar_domain_naming(self, infra1: Dict, infra2: Dict) -> bool:\n",
        "        \"\"\"Check if domain naming conventions are similar.\"\"\"\n",
        "        domains1 = infra1.get('domains', [])\n",
        "        domains2 = infra2.get('domains', [])\n",
        "        \n",
        "        for d1 in domains1:\n",
        "            for d2 in domains2:\n",
        "                # Check for similar prefixes\n",
        "                prefix1 = d1.split('.')[0].split('-')\n",
        "                prefix2 = d2.split('.')[0].split('-')\n",
        "                if set(prefix1) & set(prefix2):\n",
        "                    return True\n",
        "        return False\n",
        "    \n",
        "    def _assess_false_flag_likelihood(self, indicator_count: int) -> str:\n",
        "        \"\"\"Assess likelihood of false flag operation.\"\"\"\n",
        "        if indicator_count >= 4:\n",
        "            return \"HIGH - Strong indicators of deception\"\n",
        "        elif indicator_count >= 2:\n",
        "            return \"MEDIUM - Some inconsistencies warrant investigation\"\n",
        "        elif indicator_count >= 1:\n",
        "            return \"LOW - Minor inconsistencies noted\"\n",
        "        else:\n",
        "            return \"NONE - No deception indicators\"\n",
        "\n",
        "\n",
        "# Check for false flags\n",
        "ff_detector = FalseFlagDetector()\n",
        "\n",
        "print(\"False Flag Analysis:\")\n",
        "# CAMP-004 is designed to look like a false flag\n",
        "for campaign in CAMPAIGNS:\n",
        "    # Find similar campaigns\n",
        "    similar = [c for c in CAMPAIGNS if c.id != campaign.id \n",
        "               and ttp_analyzer.weighted_jaccard(c.ttps, campaign.ttps) > 0.5]\n",
        "    \n",
        "    if similar:\n",
        "        temporal_fp = temporal_analyzer.extract_temporal_fingerprint(campaign)\n",
        "        code_sim = code_analyzer.calculate_code_similarity(campaign, similar[0])\n",
        "        \n",
        "        result = ff_detector.analyze_for_false_flags(\n",
        "            campaign, similar, temporal_fp, code_sim\n",
        "        )\n",
        "        \n",
        "        if result['indicator_count'] > 0:\n",
        "            print(f\"\\n{campaign.name} ({campaign.id}):\")\n",
        "            print(f\"  Assessment: {result['assessment']}\")\n",
        "            print(f\"  Confidence Reduction: {result['confidence_reduction']:.0%}\")\n",
        "            for flag in result['false_flag_indicators']:\n",
        "                print(f\"  ⚠️  {flag['type']}: {flag['description']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Comprehensive Attribution Engine\n",
        "\n",
        "Combine all signals for confidence-calibrated attribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class AttributionEngine:\n",
        "    \"\"\"Comprehensive attribution with confidence calibration.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.ttp_analyzer = AdvancedTTPAnalyzer()\n",
        "        self.infra_analyzer = InfrastructureGraphAnalyzer()\n",
        "        self.temporal_analyzer = TemporalAnalyzer()\n",
        "        self.code_analyzer = CodeSimilarityAnalyzer()\n",
        "        self.ff_detector = FalseFlagDetector()\n",
        "    \n",
        "    def attribute_campaign(self, campaign: Campaign, \n",
        "                          known_actors: Dict[str, ThreatActor],\n",
        "                          related_campaigns: List[Campaign]) -> Dict:\n",
        "        \"\"\"Generate comprehensive attribution assessment.\"\"\"\n",
        "        \n",
        "        assessment = {\n",
        "            \"campaign_id\": campaign.id,\n",
        "            \"campaign_name\": campaign.name,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"matches\": [],\n",
        "            \"campaign_links\": [],\n",
        "            \"confidence\": {},\n",
        "            \"false_flag_check\": {},\n",
        "            \"recommendation\": \"\"\n",
        "        }\n",
        "        \n",
        "        # 1. Match against known actors\n",
        "        for actor_name, actor in known_actors.items():\n",
        "            match_score = self._calculate_actor_match(campaign, actor)\n",
        "            if match_score['total_score'] > 0.3:\n",
        "                assessment['matches'].append({\n",
        "                    \"actor\": actor_name,\n",
        "                    \"aliases\": actor.aliases,\n",
        "                    \"scores\": match_score,\n",
        "                    \"total_score\": match_score['total_score']\n",
        "                })\n",
        "        \n",
        "        # Sort by score\n",
        "        assessment['matches'].sort(key=lambda x: x['total_score'], reverse=True)\n",
        "        \n",
        "        # 2. Find related campaigns\n",
        "        for other in related_campaigns:\n",
        "            if other.id != campaign.id:\n",
        "                link_score = self._calculate_campaign_link(campaign, other)\n",
        "                if link_score['total_score'] > 0.4:\n",
        "                    assessment['campaign_links'].append({\n",
        "                        \"campaign_id\": other.id,\n",
        "                        \"campaign_name\": other.name,\n",
        "                        \"link_scores\": link_score\n",
        "                    })\n",
        "        \n",
        "        # 3. False flag check\n",
        "        if assessment['campaign_links']:\n",
        "            linked_campaigns = [c for c in related_campaigns \n",
        "                              if c.id in [l['campaign_id'] for l in assessment['campaign_links']]]\n",
        "            temporal_fp = self.temporal_analyzer.extract_temporal_fingerprint(campaign)\n",
        "            code_sim = self.code_analyzer.calculate_code_similarity(\n",
        "                campaign, linked_campaigns[0]\n",
        "            ) if linked_campaigns else {}\n",
        "            \n",
        "            assessment['false_flag_check'] = self.ff_detector.analyze_for_false_flags(\n",
        "                campaign, linked_campaigns, temporal_fp, code_sim\n",
        "            )\n",
        "        \n",
        "        # 4. Calculate overall confidence\n",
        "        assessment['confidence'] = self._calculate_confidence(assessment)\n",
        "        \n",
        "        # 5. Generate recommendation\n",
        "        assessment['recommendation'] = self._generate_recommendation(assessment)\n",
        "        \n",
        "        return assessment\n",
        "    \n",
        "    def _calculate_actor_match(self, campaign: Campaign, actor: ThreatActor) -> Dict:\n",
        "        \"\"\"Calculate match score against known actor.\"\"\"\n",
        "        scores = {}\n",
        "        \n",
        "        # TTP similarity\n",
        "        scores['ttp_similarity'] = self.ttp_analyzer.weighted_jaccard(\n",
        "            campaign.ttps, actor.signature_ttps\n",
        "        )\n",
        "        \n",
        "        # Target overlap\n",
        "        sector_overlap = len(set(campaign.targets['sectors']) & set(actor.target_sectors))\n",
        "        region_overlap = len(set(campaign.targets['regions']) & set(actor.target_regions))\n",
        "        scores['target_similarity'] = (\n",
        "            sector_overlap / max(len(campaign.targets['sectors']), 1) * 0.6 +\n",
        "            region_overlap / max(len(campaign.targets['regions']), 1) * 0.4\n",
        "        )\n",
        "        \n",
        "        # Infrastructure patterns\n",
        "        infra_score = 0\n",
        "        if campaign.infrastructure.get('registrar') in actor.infrastructure_patterns.get('registrars', []):\n",
        "            infra_score += 0.3\n",
        "        if campaign.infrastructure.get('hosting') in actor.infrastructure_patterns.get('hosting', []):\n",
        "            infra_score += 0.3\n",
        "        # Domain theme matching\n",
        "        for domain in campaign.infrastructure.get('domains', []):\n",
        "            for theme in actor.infrastructure_patterns.get('domain_themes', []):\n",
        "                if theme in domain:\n",
        "                    infra_score += 0.2\n",
        "                    break\n",
        "        scores['infrastructure_similarity'] = min(infra_score, 1.0)\n",
        "        \n",
        "        # Temporal pattern match\n",
        "        temporal_fp = self.temporal_analyzer.extract_temporal_fingerprint(campaign)\n",
        "        if temporal_fp:\n",
        "            expected_tz = actor.operational_tempo.get('timezone_offset', 0)\n",
        "            actual_tz = 12 - temporal_fp.get('average_hour_utc', 12)  # Rough estimate\n",
        "            tz_diff = abs(expected_tz - actual_tz)\n",
        "            scores['temporal_similarity'] = 1.0 - min(tz_diff / 12, 1.0)\n",
        "        \n",
        "        # Code signature match\n",
        "        code_score = 0\n",
        "        if campaign.code_artifacts.get('compiler') in actor.code_signatures.get('compilers', []):\n",
        "            code_score += 0.5\n",
        "        for lang in campaign.language_indicators:\n",
        "            for actor_lang in actor.code_signatures.get('language_artifacts', []):\n",
        "                if actor_lang.lower() in lang.lower():\n",
        "                    code_score += 0.5\n",
        "                    break\n",
        "        scores['code_similarity'] = min(code_score, 1.0)\n",
        "        \n",
        "        # Weighted total\n",
        "        weights = {\n",
        "            'ttp_similarity': 0.30,\n",
        "            'infrastructure_similarity': 0.25,\n",
        "            'code_similarity': 0.20,\n",
        "            'temporal_similarity': 0.15,\n",
        "            'target_similarity': 0.10\n",
        "        }\n",
        "        \n",
        "        scores['total_score'] = sum(\n",
        "            scores.get(k, 0) * v for k, v in weights.items()\n",
        "        )\n",
        "        \n",
        "        return scores\n",
        "    \n",
        "    def _calculate_campaign_link(self, c1: Campaign, c2: Campaign) -> Dict:\n",
        "        \"\"\"Calculate link strength between campaigns.\"\"\"\n",
        "        return {\n",
        "            'ttp_similarity': self.ttp_analyzer.weighted_jaccard(c1.ttps, c2.ttps),\n",
        "            'code_similarity': self.code_analyzer.calculate_code_similarity(c1, c2).get('weighted_total', 0),\n",
        "            'total_score': (\n",
        "                self.ttp_analyzer.weighted_jaccard(c1.ttps, c2.ttps) * 0.5 +\n",
        "                self.code_analyzer.calculate_code_similarity(c1, c2).get('weighted_total', 0) * 0.5\n",
        "            )\n",
        "        }\n",
        "    \n",
        "    def _calculate_confidence(self, assessment: Dict) -> Dict:\n",
        "        \"\"\"Calculate calibrated confidence levels.\"\"\"\n",
        "        base_confidence = 0.0\n",
        "        \n",
        "        if assessment['matches']:\n",
        "            best_match = assessment['matches'][0]\n",
        "            base_confidence = best_match['total_score']\n",
        "        \n",
        "        # Apply false flag reduction\n",
        "        ff_reduction = assessment.get('false_flag_check', {}).get('confidence_reduction', 0)\n",
        "        adjusted_confidence = base_confidence * (1 - ff_reduction)\n",
        "        \n",
        "        # Determine level\n",
        "        if adjusted_confidence >= 0.75:\n",
        "            level = \"HIGH\"\n",
        "        elif adjusted_confidence >= 0.50:\n",
        "            level = \"MEDIUM\"\n",
        "        elif adjusted_confidence >= 0.30:\n",
        "            level = \"LOW\"\n",
        "        else:\n",
        "            level = \"INSUFFICIENT\"\n",
        "        \n",
        "        return {\n",
        "            \"level\": level,\n",
        "            \"score\": adjusted_confidence,\n",
        "            \"base_score\": base_confidence,\n",
        "            \"adjustments\": {\n",
        "                \"false_flag_reduction\": ff_reduction\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def _generate_recommendation(self, assessment: Dict) -> str:\n",
        "        \"\"\"Generate analyst recommendation.\"\"\"\n",
        "        conf = assessment['confidence']\n",
        "        \n",
        "        if conf['level'] == \"HIGH\":\n",
        "            actor = assessment['matches'][0]['actor'] if assessment['matches'] else \"Unknown\"\n",
        "            return f\"Strong attribution to {actor}. Recommend sharing with partners.\"\n",
        "        elif conf['level'] == \"MEDIUM\":\n",
        "            return \"Moderate confidence. Additional evidence collection recommended.\"\n",
        "        elif conf['level'] == \"LOW\":\n",
        "            ff = assessment.get('false_flag_check', {})\n",
        "            if ff.get('indicator_count', 0) > 0:\n",
        "                return \"Low confidence with deception indicators. Treat attribution with caution.\"\n",
        "            return \"Low confidence. Insufficient evidence for attribution.\"\n",
        "        else:\n",
        "            return \"Attribution not possible. Collect additional IOCs and artifacts.\"\n",
        "\n",
        "\n",
        "# Run attribution engine\n",
        "engine = AttributionEngine()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ATTRIBUTION ASSESSMENT REPORTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for campaign in CAMPAIGNS:\n",
        "    result = engine.attribute_campaign(campaign, KNOWN_ACTORS, CAMPAIGNS)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Campaign: {result['campaign_name']} ({result['campaign_id']})\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    print(f\"\\nConfidence: {result['confidence']['level']} ({result['confidence']['score']:.2f})\")\n",
        "    \n",
        "    if result['matches']:\n",
        "        print(f\"\\nTop Actor Match:\")\n",
        "        top = result['matches'][0]\n",
        "        print(f\"  {top['actor']} (Score: {top['total_score']:.2f})\")\n",
        "        print(f\"    TTP: {top['scores']['ttp_similarity']:.2f}\")\n",
        "        print(f\"    Infrastructure: {top['scores']['infrastructure_similarity']:.2f}\")\n",
        "        print(f\"    Code: {top['scores']['code_similarity']:.2f}\")\n",
        "    \n",
        "    if result['false_flag_check'].get('indicator_count', 0) > 0:\n",
        "        print(f\"\\n⚠️  FALSE FLAG INDICATORS DETECTED:\")\n",
        "        print(f\"  {result['false_flag_check']['assessment']}\")\n",
        "    \n",
        "    print(f\"\\nRecommendation: {result['recommendation']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Diamond Model Report Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "def generate_diamond_model_report(campaign: Campaign, \n",
        "                                  attribution: Dict) -> str:\n",
        "    \"\"\"Generate Diamond Model analysis with LLM.\"\"\"\n",
        "    \n",
        "    client = Anthropic()\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    Generate a Diamond Model threat intelligence report for this campaign.\n",
        "\n",
        "    CAMPAIGN DATA:\n",
        "    - Name: {campaign.name}\n",
        "    - TTPs: {campaign.ttps}\n",
        "    - Malware: {campaign.malware_families}\n",
        "    - Infrastructure: {json.dumps(campaign.infrastructure)}\n",
        "    - Targets: {json.dumps(campaign.targets)}\n",
        "    - Language Indicators: {campaign.language_indicators}\n",
        "\n",
        "    ATTRIBUTION ANALYSIS:\n",
        "    - Confidence: {attribution['confidence']['level']}\n",
        "    - Top Match: {attribution['matches'][0] if attribution['matches'] else 'None'}\n",
        "    - False Flag Check: {attribution.get('false_flag_check', {})}\n",
        "\n",
        "    Create a structured Diamond Model report with:\n",
        "\n",
        "    1. ADVERSARY\n",
        "       - Assessed identity and confidence\n",
        "       - Motivation analysis\n",
        "       - Attribution evidence\n",
        "\n",
        "    2. CAPABILITY\n",
        "       - Technical sophistication assessment\n",
        "       - Malware analysis\n",
        "       - TTP analysis with MITRE mapping\n",
        "\n",
        "    3. INFRASTRUCTURE\n",
        "       - C2 infrastructure analysis\n",
        "       - Hosting and registration patterns\n",
        "       - Pivot opportunities\n",
        "\n",
        "    4. VICTIM\n",
        "       - Target profile\n",
        "       - Selection criteria assessment\n",
        "       - Predicted future targets\n",
        "\n",
        "    5. ANALYTIC CONFIDENCE\n",
        "       - Confidence assessment\n",
        "       - Alternative hypotheses\n",
        "       - Intelligence gaps\n",
        "\n",
        "    Format as a professional threat intelligence report.\n",
        "    \"\"\"\n",
        "    \n",
        "    response = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=3000,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    \n",
        "    return response.content[0].text\n",
        "\n",
        "# Uncomment to generate report (requires API key)\n",
        "# sample_attribution = engine.attribute_campaign(CAMPAIGNS[0], KNOWN_ACTORS, CAMPAIGNS)\n",
        "# report = generate_diamond_model_report(CAMPAIGNS[0], sample_attribution)\n",
        "# print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Weighted TTP Analysis**: Not all techniques are equal - rare TTPs are more distinctive\n",
        "2. **Multi-dimensional Attribution**: Combine TTPs, infrastructure, temporal, and code signals\n",
        "3. **Graph Analysis**: Infrastructure overlaps reveal campaign connections\n",
        "4. **Temporal Fingerprinting**: Operational hours indicate actor timezone\n",
        "5. **False Flag Detection**: Look for inconsistencies that indicate deception\n",
        "6. **Confidence Calibration**: Adjust confidence based on evidence quality\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- **Lab 17**: Adversarial ML - Attack and defend security models\n",
        "- **Lab 20**: LLM Red Teaming - Attack AI systems\n",
        "- **CTF Challenges**: Test attribution skills on realistic scenarios"
      ]
    }
  ]
}